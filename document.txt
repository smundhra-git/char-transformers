i will be documenting my work. Let's see how far i get. 

first what i am working on - 
I read this paper : https://arxiv.org/pdf/1706.03762 and now I am feeling like a good developer so I plan to write an entire transformer model, lets see how this macho'sm plays out.


Also I am talking to GPT on the side to breakdown the ideas i have in my head to actionable tasks (secret sauce - it works)

Okay coming back to what I am doing exactly -
I am gonna use text, possibly Shakespeare or my own code or any other text and make it predict the next character 

Let's start from 1st phase -
I will input a long stream of data (text) and then map each character to a number since our model will be using that. 

e.g. could be 'a' will be 1 and 'b' 2 and so on, and then 'A' could be 27 ...

As a weird guy, I am not using cursor right now

data/raw.txt will have the data
now i will make the Matrix, yes i am not using Eigen - keeping it as lightweight as possible and i like speed so we use C++.

In math/Matrix we will build a type matrix with dimensions

We will implement a constructor, indexing i, j = like vectors
basic sanity checks and a matmul function 

This is very basic - i have written the constructor, an access element at (row, col)
i am deciding if i want to have a conveience operator like () to call a position - i am gonna avoid it for right now. may come back later

 i have written the header file for matrix - after implementing the function i will write a Cmakefile and then eat food - feeling mexican

I am finished implementing the matrix functions and like any good developer i would test it now - you know where? In main.cpp

for cpmpilation use - g++ -std=c++17 -I./src -o main src/main.cpp src/math/matrix.cpp


I am happy to see this - 
" ./main 
=== Matrix tests start ===
Constructor / zeros / constant / fill tests passed.
add / subtract / hadamard / scalar_mul tests passed.
matmul test passed.
transpose test passed.
row_softmax tests passed.
Example matrix A:
1 2 3 
4 5 6 
=== All Matrix tests passed successfully ===
"

next we will move to phase 2 an autograd engine (Tensor + Graph) - after this our matrix will keep a track of operations and compute gradients

Alright i am back and i am gonna create the autograd engine now
Here is the design 
- We will create Tensor type that will wrap Matrix - and optionally track gradients, know which node created it (will be needed for backdrop)

- a node abstractipn representing operations in the graph- 
this will hold a reference to input tensors 
and will know how to compute gradients for inputs given gradients for outputs

- a backward(tensor& loss ) function that will start from scalar loss an walk the graph backward and fill in .grad for all parameters
this will let us do stupid but interesting things like this - 
Tensor x = Tension from matrix(A, requires a grad)
Tensor y = some operation(x, ...)
Tensor loss = mse_loss(y, target)
and loss.backward() to populare x.grad etc

now i am writing the skeleton for tensor.hpp

so i am writing it just for 2x2 right now, and given myself a TODO to expand it to N-dim. So i have made a basic skeleton for tensor and now it is time to rock node - it is interesting since i have to use a shareD_ptr<node> so i have forward declared it 

Stuf like this make me love C++ even more.

alright time ti write node's skeleton

Wrote it, later we will create a derived Node types for each of this 
- AddNode
- MatmulNode
-Relu Node and et cetra etc. 

Each derived class will store any extra info needed, and implement backward to computer gradients for the inputs

Till now this is very fun. now i am gonne implement Tensor - so i am gonna lock in for this - will write my thoughts after

I have made a stub for Tensor, Node and backward function 
first we have to design how the backprop will actaully walk the graph
implement our first differntiable op with is adding two tensors
wire the backward(loss) to call into ops' node::backward() function
and then write a test in main

before making this changes, one important thing - right now my node has a v<Tensor*> inputs and a backward() function - that means to call node->backward(), backward(loss) needs to know the grad_output but the grdaient of a node is always stored in the tensors grad object
so it will be simpler if each node has a output to its output tensor and its backward() reads output>grad internally
so first i will tweak the node a lil bit

now i will write the App for Tensors in engines/ops and here later we will have matmul, relu etc
this ops will sit between math (pure numeric) and engine(autograd)

now important notes - const_cast<Tensor*> is used because inputs will be passed as const Tensor& but we will need to mutate .grad - this is a common design for autograd - we rely that the user will not modify the data via non-const ops
we also ensutre that only tensors with requires_grad = true are updated

now we will implement the real backward(Tensor& loss) graph traversal
- currently we have tensors with .grad_fn and nodes with input,ouput and backward()
we need a topological order of this nodes from the loss (downwards)
then traverse them in reverse and continuosly propogate the gradients 

so easiest thing which comes to mind directly here is DFS from loss.grad_fn to collect all the reachable nodes, sotre them in a vector post-order - reverse the vector and we can get the backprop order

so i just wrote the DFS, remembered my competitve programming days - it took me less than 40 secs to write it

well now i have also implemented the sum feautes and the main is now tests for that. 
the next steps will be to implement more ops like matmul, elementwise nonlibearity (relu) etc
and build a small MLP on top

now sinec we have a baby autograd engine 
we will make a neural network primitive at the Tensor level - what this means to common people like us is we will write the matmul function at node level and an activation function, also a simple loss

well i have implemented relu and matmul, also during this time i read a paper
- https://pmc.ncbi.nlm.nih.gov/articles/PMC11142305/
good paper, i will beat the stats there


by the way tested my relu, matmul and loss and guess what? i got a seegmentation fault error

Thought - "He cut his hand" is a sentence filled with linguistic ambuigity, if someday humankind cracks AGI what would it assume as most of the time, did A cut B hands or A cuts A hand?


the seg fault was insane, i was directly passing the output of matmul to relu - and not creating a variable to store it and hence it crashed

now we will work on the linear layer abstraction y=xW + b 
a basic loss + optimizer
and a tiny mlp sanity check 

I am starting with Linear now
theoritically this is what linear will do - for input batch x of shape [bs x if]:
y = xW + b 
W is [if*of]
b is [1xof] and y is [bs * of]
bs is batch size, if is in features and of is guess what? out fesutres. 

well i locked in and implemented the Linear implementation, one hting which you may face too is if you have a node which you wil ldestruct future do nto keep it as a ref to another node or else the program will crash 

now we will add a mse_loss op and a tiny SGD optimizer, wire a 2 layer MLP (on say XOR) and run a end-to-end test

Writing the MSE was not fun (not if you have notices the trend that the information repeats itself in the node extensions now) so it is a lot of not thinking jsut doing now. 

also another interesting paper i read last night was - https://arxiv.org/pdf/2506.12220. Yes there are some flaws in the paper and i have informed the authors, i hop they improve them

And now getting back to writing the optimizer or the sgd 

so i have made the header file and this shall be very simple to implement 

one thing i have to do is ensure that i do not keep accessing deleted memory, get seg fault error and then fix - it is wasting my time

amazingly the optimizer worked in a single time and now i am gonna build a char-level transformer on top of this engine (embedding -> multi head attention -> MLP -> output head) and later i will evolve this into a medical reasoning transformer.

before that I tried to tweak the test to XOR and it didn't really learn - interesting - have to tweak the mse 

=== 2-layer MLP on XOR (1-sample SGD) ===
Epoch 0  loss = 0.44023
Epoch 500  loss = 1.23689e-22
Epoch 1000  loss = 1.97215e-31
Epoch 1500  loss = 1.97215e-31
Epoch 2000  loss = 1.97215e-31
Epoch 2500  loss = 1.97215e-31
Epoch 3000  loss = 1.97215e-31
Epoch 3500  loss = 1.97215e-31
Epoch 4000  loss = 1.97215e-31
Epoch 4500  loss = 1.97215e-31

Final predictions:
(0, 0) -> 4.44089e-16 (target 0)
(0, 1) -> 1 (target 1)
(1, 0) -> 1 (target 1)
(1, 1) -> 3.9968e-16 (target 0)

this is the response i got. Isn't it beautiful? It is textbook XOR solve.

now i am gonna push this and then start working on data + encoding utilies, positional encoding modules and self attention blocks

first target we have is to make a tiny char-transfoemr spec 
few numbers we need to know 
the dimensions of the model is 64, number of heads is 4, layers is 2, block size will be 64 and vocab size would be whatever the text gives (i aim to start close to 100 chars)

now we will be dealing with strings, lets see how much solving strings question on leetcode will help me. refer to src/data for this

so i have implemented both vocab and datasets and am now wriing tests, after writing a bit you should always test

next step we will make a mebeddging layer design 
what embeddings will do - 
Param W with shape [vocab size * d model], requires_grad = true
input a vector<int> of token ids and output a tensor of shape [n*d_model] for each position p with id token_ids[p] = i we will accumulate as so 
dL/dWi += dL/Doutp - the node will store a copy of the ids inside itself to it does not scatter-add in backward

so i did that step and tested it and it is working awesonly 


next step is to have positional embeddings + reshaping, then i will build a single self-attention block and then stack a couple of those, finally build a Linear head for next char prediction 

so if you remember we had written a row_softmax for Matrix, and now we are gonna write one for Tensor too 

well that part is done and i tested it too; it works as intended. 

also i read this - https://arxiv.org/pdf/2511.21522, and combined with the paper of AI Supply chain, I believe could make a complete cycle of trust in AI workflow which includes collecting data, labelling data - feeding into a neural net - getting responses and improving each step based on that, that could be an expansion from here but first would be the medical reasoning model 

coming back, now we are ready to climb into seld-attention
to do this, we will need two things - a tensor level transpose so we can do score - Q * K^t

and a first self attension module that uses existing ops 

first i am gonna write the tensor level traponse - thats done now 

now i will build a single-head attention block that : Takes X of Shape [T * d_model] - sequence lentg T and embedding dimension 

Computes the following 
Q = X W_q
K = X W_k
V = X W_v
scores = Q * K^t/sqrt(d_k) {d_k = d_model for now}
apply casual mask
P = softmax_rows(scores)
Y = P * V 
out = Y W_o

now a question to arise here is why am i doing this set? for that read the attension if all you need paper linked above

i was testing this and gor some eerror and deeper investigation i would classify myself as lucky till now because my pointers were leaving in the outer scope (main) and hence not dieing but now i have to refactor the code to introduce tensorbody and make tensor a handle

i refctored the code, and now will be following the new ownership of Tensor and TensorBody - i also tested tested the self attention and it works

now we will wrap it all in the transformer block 
X - > SelfAttention -> residual -> MLP (Linear, ReLU -> Linear) -> Residial 

then we will add a final output head Linear(d->model : Voab Size)
implement cross entropy loss over next char prediction 
and then test on real texts

so lets write the full transformer block 
we will keep single head, batch = 1. no layerform for v1
so X E (contains)  R ^ (T * d_model)

the transformer block works, and ow i will add an LM head + a cross entory loss op so that we can turn the [T * d_model] into [T * vocab size logits and train]

now i am gonna wire this to my chardataset side and actually predict next chars

well now i am putting the whole thing together and writing a test, will just use a tiny fake vocab of size 8

now we will move from the toy to a real cahr level lm on text

i have refactored the entire code to use n-dim tensor engine similar to pytorch

Word-level + Sinusoidal PE:
I have now updated the repository to follow "Attention Is All You Need" more closely, since i watched this video to get a better grasp - https://www.youtube.com/watch?v=bCz4OMemCcA&t=26s - i would highly suggest watching it but again its upto you:
the first thing i moved from char-lebel to world-level tokenization, learning words is cooler 
secondly i incoporated positional encoding using the sine and cosine functions which our dear own Google Brain people suggested, updated the transformer to use this new positional encoding 

lets run some tests 
well the loss function stayed > 3.4 

now when i was starting my research in the field of AI i read this basic article on geeks (99th percentile site fosho) - https://www.geeksforgeeks.org/deep-learning/adam-optimizer/

and npw i will use it to optimize by loss, now i switched from SGD to Adam in main and i got a sub 0.003 loss 

now i am happy and will test my program on something longer after pushing this to git

wait before pushing i will write a quick Readme to redirect people here